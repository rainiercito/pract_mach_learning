---
title: "Practical Machine Learning Project"
author: "Victor Rainier Cruz Perez"
date: "Saturday, November 15, 2014"
output: html_document
---

## Background

A group of enthusiasts who take measurements about themselves for improving their health uses devices such as Jawbone Up, Nike FuelBand, and Fitbit to collect a large amount of data about personal activity relatively inexpensively. People regularly  quantifies how much of a particular activity they do but rarely quantify how well they do it. 

The participants (6) were asked to perform barbell lifts correctly and incorrectly in 5 different ways wearing accelerometers on the belt, forearm, arm, and dumbell.

More information visit the website: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data Machine Learning Project Objetive


Predict the manner in which the participants did the exercise. This is the "classe" variable in the training set. One may use any of the other variables to predict with. The report should include the description of how the model was build, how the cross validation was used, and the sample error expentations might be. One should use a prediction model to predict 20 different test cases. 

Also the prediction submition is required considering the format specifications requested for the automated-programming-assignment grading. See the programming assignment for additional details.

## Data Sources

The training and test data are available at: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The orignial data source can be found at: http://groupware.les.inf.puc-rio.br/har.


## Considerations 

The presence of N/A values should be reviewed before perform any data analysis. Also, to assure the reproduction of results it is necessary to set a pseudo random seed. 

The libraries used for the project are: 
```{r, echo=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

The selected seed for is project is:
```{r}
set.seed(54321)
```

## Getting the data

Downloading the data and store it on the computer's memory, tagging the NA values contained in both data sets

```{r}
Urltrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
Urltest <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

traininginfo <- read.csv(url(Urltrain), na.strings=c("NA","#DIV/0!",""))
testinginfo <- read.csv(url(Urltest), na.strings=c("NA","#DIV/0!",""))
```

##Partioning the training set into two

Partion the Training data set in two sets (70% Training, 30% Testing):

```{r, echo=FALSE}
inTrain <- createDataPartition(y=traininginfo$classe, p=0.7, list=FALSE)
workingTraining <- traininginfo[inTrain, ]
workingTest <- traininginfo[-inTrain, ]
dim(workingTraining)
dim(workingTest)
```


## Cleaning the data

Applying the  Near Zero Variance transformation
```{r}
workingDataNZV <- nearZeroVar(workingTraining, saveMetrics=TRUE)
workingNZVvariables <- names(workingTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
workingTraining <- workingTraining[!workingNZVvariables]
#Reviewing the new training data set size
dim(workingTraining)
```

Removing both the ID Column and the NA values from the dataset:
```{r}
workingTraining <- workingTraining[c(-1)]

tempworktrainig <- workingTraining
for(i in 1:length(workingTraining)) 
{
        if( sum( is.na( workingTraining[, i] ) ) /nrow(workingTraining) >= .55 )
          { #if the number of NA per row is more than 55% of total observations we remove the entire row
		        for(j in 1:length(tempworktrainig))
              {
			          if( length( grep(names(workingTraining[i]), names(tempworktrainig)[j]) ) ==1) 
                  { #if we found duplicity in the columns, we remove one
				        tempworktrainig <- tempworktrainig[ , -j] #Remove the duplicated column
			            }	
		          } 
	        }
}
#Determine the cleaning dataset
dim(tempworktrainig)

#Re-utilizing my training variable name and removing the cleaning dataset
workingTraining <- tempworktrainig
rm(tempworktrainig)
```

Appliying the same transformations to the test dataset:

```{r}
clean1 <- colnames(workingTraining)
clean2 <- colnames(workingTraining[, -58]) #already with classe column removed
workingTest <- workingTest[clean1]
testinginfo <- testinginfo[clean2]

#Determine the cleaning dataset size
dim(workingTest); dim(testinginfo)
```

For the correct use of the RandomForest Algorithm over the Test Data the data must be the same type:

```{r}
for (i in 1:length(testinginfo) )
  {
        for(j in 1:length(workingTraining)) 
          {
		      if( length( grep(names(workingTraining[i]), names(testinginfo)[j]) ) ==1)
            {
			      class(testinginfo[j]) <- class(workingTraining[i])
		        }      
	        }      
  }
#Removing the row two that is not need it
testinginfo <- rbind(workingTraining[2, -58] , testinginfo) 
testinginfo <- testinginfo[-1,]
```

## Using Decision Tree as Predictor Model

```{r}
modFitA1 <- rpart(classe ~ ., data=workingTraining, method="class")
fancyRpartPlot(modFitA1)
```

Executing the prediction:

```{r}
predictionsA1 <- predict(modFitA1, workingTest, type = "class")
```

Using the ConfusionMatrix to test the results
```{r, echo=TRUE}
confusionMatrix(predictionsA1, workingTest$classe)
```

## Using Random Forests as Prediction Model

```{r}
modFitB1 <- randomForest(classe ~. , data=workingTraining)
```

Predicting and review the comparisson by using the ConfusionMatrix Function:
```{r}
predictionsB1 <- predict(modFitB1, workingTest, type = "class")
confusionMatrix(predictionsB1, workingTest$classe)
```

From the results showed above, one can observe that the Random forest express more accurate results.

## Generating Files to submit as answers for the Assignment:

```{r}
predictionsB2 <- predict(modFitB1, testinginfo, type = "class")
```

Function to generate files with predictions to submit for assignment
```{r}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsB2)
```
